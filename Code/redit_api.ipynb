{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc4602f7",
   "metadata": {},
   "source": [
    "# Reddit API Data Collection & Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9912a78",
   "metadata": {},
   "source": [
    "## Part 1: Reddit API Setup & Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7481559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to Reddit API!\n",
      "Testing with subreddit: test\n",
      "Reddit API connection successful!\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "from credentials import client_id, client_secret, user_agent\n",
    "\n",
    "# Create Reddit instance (read-only access)\n",
    "reddit = praw.Reddit(\n",
    "    client_id=client_id,\n",
    "    client_secret=client_secret,\n",
    "    user_agent=user_agent\n",
    ")\n",
    "\n",
    "# Test the connection\n",
    "try:\n",
    "    # Test by accessing a subreddit (read-only operation)\n",
    "    subreddit = reddit.subreddit(\"test\")\n",
    "    print(f\"Successfully connected to Reddit API!\")\n",
    "    print(f\"Testing with subreddit: {subreddit.display_name}\")\n",
    "    print(\"Reddit API connection successful!\")\n",
    "except Exception as e:\n",
    "    print(f\"Connection failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e41696",
   "metadata": {},
   "source": [
    "## Part 2: Collect Data and Storage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2896534d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "COLLECTING REDDIT DATA FOR SENTIMENT ANALYSIS\n",
      "============================================================\n",
      "\n",
      "üìä Collecting data from r/politics...\n",
      "   ‚úÖ Successfully retrieved 20 posts\n",
      "\n",
      "üìä Collecting data from r/PoliticalDiscussion...\n",
      "   ‚úÖ Successfully retrieved 20 posts\n",
      "\n",
      "üìä Collecting data from r/PoliticalDiscussion...\n",
      "   ‚úÖ Successfully retrieved 20 posts\n",
      "\n",
      "üìä Collecting data from r/worldnews...\n",
      "   ‚úÖ Successfully retrieved 20 posts\n",
      "\n",
      "üìä Collecting data from r/worldnews...\n",
      "   ‚úÖ Successfully retrieved 20 posts\n",
      "\n",
      "üìà DATA COLLECTION SUMMARY:\n",
      "   Total posts collected: 60\n",
      "   Posts per subreddit:\n",
      "     r/politics: 20 posts\n",
      "     r/PoliticalDiscussion: 20 posts\n",
      "     r/worldnews: 20 posts\n",
      "   ‚úÖ Successfully retrieved 20 posts\n",
      "\n",
      "üìà DATA COLLECTION SUMMARY:\n",
      "   Total posts collected: 60\n",
      "   Posts per subreddit:\n",
      "     r/politics: 20 posts\n",
      "     r/PoliticalDiscussion: 20 posts\n",
      "     r/worldnews: 20 posts\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Target subreddits as specified in assignment\n",
    "target_subreddits = ['politics', 'PoliticalDiscussion', 'worldnews']\n",
    "posts_per_subreddit = 20\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"COLLECTING REDDIT DATA FOR SENTIMENT ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Initialize list to store all posts data\n",
    "all_posts_data = []\n",
    "\n",
    "# Collect data from each subreddit\n",
    "for subreddit_name in target_subreddits:\n",
    "    print(f\"\\nüìä Collecting data from r/{subreddit_name}...\")\n",
    "    \n",
    "    try:\n",
    "        # Access the subreddit\n",
    "        subreddit = reddit.subreddit(subreddit_name)\n",
    "        \n",
    "        # Get hot posts (you can change to .top() if needed)\n",
    "        posts = list(subreddit.hot(limit=posts_per_subreddit))\n",
    "        \n",
    "        print(f\"   ‚úÖ Successfully retrieved {len(posts)} posts\")\n",
    "        \n",
    "        # Extract required data for each post\n",
    "        for post in posts:\n",
    "            post_data = {\n",
    "                'subreddit': subreddit_name,\n",
    "                'title': post.title,\n",
    "                'score': post.score,\n",
    "                'num_comments': post.num_comments,\n",
    "                'id': post.id,\n",
    "                'url': post.url,\n",
    "                'created_utc': datetime.fromtimestamp(post.created_utc),\n",
    "                'author': str(post.author) if post.author else '[deleted]'\n",
    "            }\n",
    "            all_posts_data.append(post_data)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error collecting from r/{subreddit_name}: {e}\")\n",
    "\n",
    "# Create DataFrame for storage\n",
    "df_posts = pd.DataFrame(all_posts_data)\n",
    "\n",
    "print(f\"\\nüìà DATA COLLECTION SUMMARY:\")\n",
    "print(f\"   Total posts collected: {len(df_posts)}\")\n",
    "print(f\"   Posts per subreddit:\")\n",
    "for subreddit in target_subreddits:\n",
    "    count = len(df_posts[df_posts['subreddit'] == subreddit])\n",
    "    print(f\"     r/{subreddit}: {count} posts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f77fa82",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_filename = \"reddit_posts_data.csv\"\n",
    "df_posts.to_csv(csv_filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d7330c",
   "metadata": {},
   "source": [
    "## Part 3: Comment Collection from Most Relevant Posts\n",
    "\n",
    "**Objective**: Collect 5 comments per post from the most relevant posts (highest score + engagement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8329c733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "COLLECTING COMMENTS FROM MOST RELEVANT POSTS\n",
      "============================================================\n",
      "üìä Selected 10 most relevant posts for comment collection:\n",
      "   Selection criteria: score + (num_comments √ó 2)\n",
      "\n",
      "Top Posts Selected:\n",
      "   1. r/worldnews: Zelenskyy points out that Trump‚Äôs ‚Äútwo weeks‚Äù give... (Score: 36367, Comments: 681)\n",
      "   2. r/politics: Trump faces returning $100bn in tariffs after cour... (Score: 29351, Comments: 1176)\n",
      "   3. r/politics: Bernie Sanders breaks with Democrats and endorses ... (Score: 21670, Comments: 676)\n",
      "   4. r/worldnews: EU head's plane hit by suspected Russian GPS inter... (Score: 19752, Comments: 665)\n",
      "   5. r/politics: Donald Trump posting week-old photo raises eyebrow... (Score: 15378, Comments: 1434)\n",
      "   6. r/PoliticalDiscussion: Casual Questions Thread... (Score: 89, Comments: 8311)\n",
      "   7. r/worldnews: To defend against Russian tanks, Finland and Polan... (Score: 7082, Comments: 240)\n",
      "   8. r/politics: Donald Trump is weaker than he looks... (Score: 6050, Comments: 537)\n",
      "   9. r/worldnews: All UN Security Council members, except US, say fa... (Score: 5861, Comments: 622)\n",
      "   10. r/worldnews: Ukraine plans new strikes deep into Russia, Zelens... (Score: 5811, Comments: 120)\n",
      "\n",
      "üîç Collecting 5 comments per post...\n",
      "\n",
      "   üìù Processing post 1/10: r/worldnews\n",
      "      ‚úÖ Collected 5 comments\n",
      "\n",
      "   üìù Processing post 2/10: r/politics\n",
      "      ‚úÖ Collected 5 comments\n",
      "\n",
      "   üìù Processing post 2/10: r/politics\n",
      "      ‚úÖ Collected 5 comments\n",
      "\n",
      "   üìù Processing post 3/10: r/politics\n",
      "      ‚úÖ Collected 5 comments\n",
      "\n",
      "   üìù Processing post 3/10: r/politics\n",
      "      ‚úÖ Collected 5 comments\n",
      "\n",
      "   üìù Processing post 4/10: r/worldnews\n",
      "      ‚úÖ Collected 5 comments\n",
      "\n",
      "   üìù Processing post 4/10: r/worldnews\n",
      "      ‚úÖ Collected 5 comments\n",
      "\n",
      "   üìù Processing post 5/10: r/politics\n",
      "      ‚úÖ Collected 5 comments\n",
      "\n",
      "   üìù Processing post 5/10: r/politics\n",
      "      ‚úÖ Collected 5 comments\n",
      "\n",
      "   üìù Processing post 6/10: r/PoliticalDiscussion\n",
      "      ‚úÖ Collected 5 comments\n",
      "\n",
      "   üìù Processing post 6/10: r/PoliticalDiscussion\n",
      "      ‚úÖ Collected 5 comments\n",
      "\n",
      "   üìù Processing post 7/10: r/worldnews\n",
      "      ‚úÖ Collected 5 comments\n",
      "\n",
      "   üìù Processing post 7/10: r/worldnews\n",
      "      ‚úÖ Collected 5 comments\n",
      "\n",
      "   üìù Processing post 8/10: r/politics\n",
      "      ‚úÖ Collected 5 comments\n",
      "\n",
      "   üìù Processing post 8/10: r/politics\n",
      "      ‚úÖ Collected 5 comments\n",
      "\n",
      "   üìù Processing post 9/10: r/worldnews\n",
      "      ‚úÖ Collected 5 comments\n",
      "\n",
      "   üìù Processing post 9/10: r/worldnews\n",
      "      ‚úÖ Collected 5 comments\n",
      "\n",
      "   üìù Processing post 10/10: r/worldnews\n",
      "      ‚úÖ Collected 5 comments\n",
      "\n",
      "   üìù Processing post 10/10: r/worldnews\n",
      "      ‚úÖ Collected 5 comments\n",
      "\n",
      "üìà COMMENT COLLECTION SUMMARY:\n",
      "   Total comments collected: 50\n",
      "   Comments per subreddit:\n",
      "     r/worldnews: 25 comments\n",
      "     r/politics: 20 comments\n",
      "     r/PoliticalDiscussion: 5 comments\n",
      "\n",
      "üìã Sample of collected comments:\n",
      "   subreddit  score                                               body\n",
      "0  worldnews   5776        Is it the third or the fourth \"two weeks\" ?\n",
      "1  worldnews   1589                    \"Day one\" is also a while back.\n",
      "2  worldnews   1219  Gonna call Zelenskyy nasty for his ability to ...\n",
      "      ‚úÖ Collected 5 comments\n",
      "\n",
      "üìà COMMENT COLLECTION SUMMARY:\n",
      "   Total comments collected: 50\n",
      "   Comments per subreddit:\n",
      "     r/worldnews: 25 comments\n",
      "     r/politics: 20 comments\n",
      "     r/PoliticalDiscussion: 5 comments\n",
      "\n",
      "üìã Sample of collected comments:\n",
      "   subreddit  score                                               body\n",
      "0  worldnews   5776        Is it the third or the fourth \"two weeks\" ?\n",
      "1  worldnews   1589                    \"Day one\" is also a while back.\n",
      "2  worldnews   1219  Gonna call Zelenskyy nasty for his ability to ...\n"
     ]
    }
   ],
   "source": [
    "# STEP 1: Identify most relevant posts based on engagement metrics\n",
    "print(\"=\"*60)\n",
    "print(\"COLLECTING COMMENTS FROM MOST RELEVANT POSTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Calculate engagement score (combination of score and comments)\n",
    "df_posts['engagement_score'] = df_posts['score'] + (df_posts['num_comments'] * 2)\n",
    "\n",
    "# Sort by engagement score and select top posts for comment collection\n",
    "# Let's take top 10 most relevant posts across all subreddits\n",
    "top_posts = df_posts.nlargest(10, 'engagement_score')\n",
    "\n",
    "print(f\"üìä Selected {len(top_posts)} most relevant posts for comment collection:\")\n",
    "print(f\"   Selection criteria: score + (num_comments √ó 2)\")\n",
    "print(\"\\nTop Posts Selected:\")\n",
    "for idx, (_, post) in enumerate(top_posts.iterrows(), 1):\n",
    "    print(f\"   {idx}. r/{post['subreddit']}: {post['title'][:50]}... (Score: {post['score']}, Comments: {post['num_comments']})\")\n",
    "\n",
    "# STEP 2: Collect comments from these top posts\n",
    "comments_per_post = 5\n",
    "all_comments_data = []\n",
    "\n",
    "print(f\"\\nüîç Collecting {comments_per_post} comments per post...\")\n",
    "\n",
    "for idx, (_, post) in enumerate(top_posts.iterrows(), 1):\n",
    "    print(f\"\\n   üìù Processing post {idx}/{len(top_posts)}: r/{post['subreddit']}\")\n",
    "    \n",
    "    try:\n",
    "        # Get the Reddit submission object using the post ID\n",
    "        submission = reddit.submission(id=post['id'])\n",
    "        \n",
    "        # Replace \"MoreComments\" objects and get top-level comments\n",
    "        submission.comments.replace_more(limit=0)\n",
    "        \n",
    "        # Get the top comments (sorted by score)\n",
    "        top_comments = sorted(submission.comments, key=lambda x: x.score, reverse=True)\n",
    "        \n",
    "        # Collect up to 5 comments per post\n",
    "        comments_collected = 0\n",
    "        for comment in top_comments:\n",
    "            if comments_collected >= comments_per_post:\n",
    "                break\n",
    "                \n",
    "            # Skip deleted/removed comments\n",
    "            if hasattr(comment, 'body') and comment.body not in ['[deleted]', '[removed]']:\n",
    "                comment_data = {\n",
    "                    'post_id': post['id'],\n",
    "                    'post_title': post['title'],\n",
    "                    'subreddit': post['subreddit'],\n",
    "                    'comment_id': comment.id,\n",
    "                    'body': comment.body,\n",
    "                    'score': comment.score,\n",
    "                    'author': str(comment.author) if comment.author else '[deleted]',\n",
    "                    'created_utc': datetime.fromtimestamp(comment.created_utc)\n",
    "                }\n",
    "                all_comments_data.append(comment_data)\n",
    "                comments_collected += 1\n",
    "        \n",
    "        print(f\"      ‚úÖ Collected {comments_collected} comments\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"      ‚ùå Error collecting comments from post {post['id']}: {e}\")\n",
    "\n",
    "# STEP 3: Create DataFrame and save comments data\n",
    "df_comments = pd.DataFrame(all_comments_data)\n",
    "\n",
    "print(f\"\\nüìà COMMENT COLLECTION SUMMARY:\")\n",
    "print(f\"   Total comments collected: {len(df_comments)}\")\n",
    "print(f\"   Comments per subreddit:\")\n",
    "for subreddit in df_comments['subreddit'].unique():\n",
    "    count = len(df_comments[df_comments['subreddit'] == subreddit])\n",
    "    print(f\"     r/{subreddit}: {count} comments\")\n",
    "\n",
    "# Display sample of collected comments\n",
    "print(f\"\\nüìã Sample of collected comments:\")\n",
    "print(df_comments[['subreddit', 'score', 'body']].head(3).to_string(max_colwidth=50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bcaece20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ STORAGE COMPLETED:\n",
      "   Comments saved to: reddit_comments_data.csv\n",
      "   Total records: 50\n",
      "   Columns: ['post_id', 'post_title', 'subreddit', 'comment_id', 'body', 'score', 'author', 'created_utc']\n",
      "\n",
      "üîó DATA INTEGRITY CHECK:\n",
      "   Comments are linked to 10 unique posts\n",
      "   Average comments per post: 5.0\n",
      "\n",
      "üìä DATA STRUCTURE PREVIEW:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50 entries, 0 to 49\n",
      "Data columns (total 8 columns):\n",
      " #   Column       Non-Null Count  Dtype         \n",
      "---  ------       --------------  -----         \n",
      " 0   post_id      50 non-null     object        \n",
      " 1   post_title   50 non-null     object        \n",
      " 2   subreddit    50 non-null     object        \n",
      " 3   comment_id   50 non-null     object        \n",
      " 4   body         50 non-null     object        \n",
      " 5   score        50 non-null     int64         \n",
      " 6   author       50 non-null     object        \n",
      " 7   created_utc  50 non-null     datetime64[ns]\n",
      "dtypes: datetime64[ns](1), int64(1), object(6)\n",
      "memory usage: 3.3+ KB\n",
      "None\n",
      "\n",
      "‚úÖ Comment collection and storage completed successfully!\n",
      "   Each comment is properly linked to its parent post via 'post_id'\n",
      "   Ready for sentiment analysis in next steps\n"
     ]
    }
   ],
   "source": [
    "# STEP 4: Save comments data with proper linking to parent posts\n",
    "comments_csv_filename = \"reddit_comments_data.csv\"\n",
    "df_comments.to_csv(comments_csv_filename, index=False)\n",
    "\n",
    "print(f\"\\nüíæ STORAGE COMPLETED:\")\n",
    "print(f\"   Comments saved to: {comments_csv_filename}\")\n",
    "print(f\"   Total records: {len(df_comments)}\")\n",
    "print(f\"   Columns: {list(df_comments.columns)}\")\n",
    "\n",
    "# Verify data integrity - ensure all comments are linked to valid posts\n",
    "linked_posts = df_comments['post_id'].nunique()\n",
    "print(f\"\\nüîó DATA INTEGRITY CHECK:\")\n",
    "print(f\"   Comments are linked to {linked_posts} unique posts\")\n",
    "print(f\"   Average comments per post: {len(df_comments) / linked_posts:.1f}\")\n",
    "\n",
    "# Show data structure for verification\n",
    "print(f\"\\nüìä DATA STRUCTURE PREVIEW:\")\n",
    "print(df_comments.info())\n",
    "\n",
    "print(f\"\\n‚úÖ Comment collection and storage completed successfully!\")\n",
    "print(f\"   Each comment is properly linked to its parent post via 'post_id'\")\n",
    "print(f\"   Ready for sentiment analysis in next steps\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Data-Science",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
